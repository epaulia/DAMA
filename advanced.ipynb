{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "601b158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb54956",
   "metadata": {},
   "source": [
    "DATA LOADING AND INITIAL ASSESSMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0f359671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (27000, 18)\n",
      "Dataset contains 27,000 transactions across 18 features\n",
      "\n",
      "Missing values per column:\n",
      "price_doc              0\n",
      "full_sq                0\n",
      "life_sq             5537\n",
      "kitch_sq            9572\n",
      "floor                167\n",
      "max_floor           9572\n",
      "build_year         12869\n",
      "num_room            9572\n",
      "state              13067\n",
      "product_type           0\n",
      "ecology                0\n",
      "sub_area               0\n",
      "raion_popul            0\n",
      "kindergarten_km        0\n",
      "school_km              0\n",
      "park_km                0\n",
      "railroad_km            0\n",
      "metro_min_walk        14\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "house_data = pd.read_csv(\"Data/sberbank_housing.csv\", index_col=0, low_memory=True)\n",
    "\n",
    "# Drop non-predictive columns\n",
    "house_data = house_data.drop(columns=[\"timestamp\", \"id\"])\n",
    "\n",
    "# Assess dataset dimensions and data quality\n",
    "print(f\"Original dataset shape: {house_data.shape}\")\n",
    "print(f\"Dataset contains {house_data.shape[0]:,} transactions across {house_data.shape[1]} features\")\n",
    "\n",
    "# Missing value analysis\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(house_data.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd458b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transform to target (prices are heavily right-skewed)\n",
    "house_data[\"price_doc\"] = np.log1p(house_data[\"price_doc\"])\n",
    "\n",
    "# We checked that previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5e02abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the feature distributions\n",
    "\n",
    "# columns_to_inspect = house_data.drop('price_doc',axis=1).select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# # Let's look at histograms\n",
    "# nrows=3\n",
    "# ncols=6\n",
    "# fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 12))\n",
    "# axes_flat = axes.flatten()\n",
    "\n",
    "# for ax, col in zip(axes_flat, columns_to_inspect):\n",
    "    \n",
    "#     ax.hist(house_data[col], bins=50)\n",
    "#     ax.set_title(col)\n",
    "    \n",
    "# for ax in axes_flat[len(columns_to_inspect):]:\n",
    "#     ax.set_visible(False)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "43fbc2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove obvious errors and impossible values\n",
    "house_data = house_data[house_data[\"full_sq\"] <= 1200]  # Remove extreme outliers for full square footage\n",
    "house_data = house_data[house_data[\"life_sq\"] <= 1000] # Remove extreme outliers for living space square footage\n",
    "house_data = house_data[house_data[\"kitch_sq\"] <= 1000] # Remove extreme outliers for living space square footage\n",
    "house_data = house_data[house_data[\"state\"] < 10]\n",
    "house_data = house_data[house_data[\"num_room\"] <= 15]   # Remove 'impossible' room counts\n",
    "house_data = house_data[house_data[\"build_year\"] > 1800]  # Remove 'impossible' years\n",
    "house_data = house_data.drop_duplicates()               # Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b186b",
   "metadata": {},
   "source": [
    "SPLIT DATA FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e28de7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (9460, 17)\n",
      "Test set: (2366, 17)\n"
     ]
    }
   ],
   "source": [
    "X = house_data.drop(\"price_doc\", axis=1)\n",
    "y = house_data[\"price_doc\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c5ae5",
   "metadata": {},
   "source": [
    "FOCUSED PRACTICAL PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4d6c9",
   "metadata": {},
   "source": [
    "The HybridImputer uses two different imputation strategies based on data type, because numeric and categorical missing values have different optimal solutions.\n",
    "\n",
    "LEARNING PHASE (fit method):\n",
    "\n",
    "Categorical - self.modes['sub_area'] = 'Downtown'  # Most common neighborhood\n",
    "\n",
    "Numeric - Training data with missing 'life_sq' values --> KNN looks at similar houses to predict missing living space --> Finds 5 most similar houses by distance and averages their life_sq values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45786de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"KNN for numeric, mode for categorical\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.knn_imputer = KNNImputer(n_neighbors=5, weights=\"distance\")\n",
    "        self.modes = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Handle categorical first\n",
    "        cat_cols = X.select_dtypes(include=['object']).columns\n",
    "        for col in cat_cols:\n",
    "            self.modes[col] = X[col].mode().iloc[0] if len(X[col].mode()) > 0 else 'unknown'\n",
    "        \n",
    "        # Fit KNN on numeric columns only\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            self.knn_imputer.fit(X[numeric_cols])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_work = X.copy()\n",
    "        \n",
    "        # Fill categorical with modes first\n",
    "        for col, mode_val in self.modes.items():\n",
    "            if col in X_work.columns:\n",
    "                X_work[col] = X_work[col].fillna(mode_val)\n",
    "        \n",
    "        # KNN imputation for numeric\n",
    "        numeric_cols = X_work.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            X_work[numeric_cols] = self.knn_imputer.transform(X_work[numeric_cols])\n",
    "        \n",
    "        return X_work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe43b1",
   "metadata": {},
   "source": [
    "The SelectiveFeatureEngineer creates new, highly predictive features by combining existing raw features in meaningful ways. It's like a domain expert extracting insights that a model can't discover on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ca11ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Only the most impactful feature engineering\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_work = X.copy()\n",
    "        epsilon = 1e-5\n",
    "        \n",
    "        # Only add features that are likely to be very predictive\n",
    "        \n",
    "        # 1. Living space efficiency (strong predictor of price per sqft)\n",
    "        if 'life_sq' in X_work.columns and 'full_sq' in X_work.columns:\n",
    "            X_work['living_efficiency'] = X_work['life_sq'] / (X_work['full_sq'] + epsilon)\n",
    "        \n",
    "        # 2. Room size (bigger rooms = luxury)\n",
    "        if 'full_sq' in X_work.columns and 'num_room' in X_work.columns:\n",
    "            X_work['avg_room_size'] = X_work['full_sq'] / (X_work['num_room'] + epsilon)\n",
    "        \n",
    "        # 3. Floor desirability (middle floors often preferred)\n",
    "        if 'floor' in X_work.columns and 'max_floor' in X_work.columns:\n",
    "            X_work['floor_ratio'] = X_work['floor'] / (X_work['max_floor'] + epsilon)\n",
    "        \n",
    "        # 4. Overall area (log transform to handle skewness)\n",
    "        if 'full_sq' in X_work.columns:\n",
    "            X_work['log_full_sq'] = np.log1p(X_work['full_sq'])\n",
    "        \n",
    "        # 5. Overall amenity accessibility score\n",
    "        amenity_cols = ['kindergarten_km', 'school_km', 'park_km', 'metro_min_walk']\n",
    "        available_amenities = [col for col in amenity_cols if col in X_work.columns]\n",
    "        if len(available_amenities) >= 2:\n",
    "            # Inverse distance (closer = higher score)\n",
    "            X_work['amenity_score'] = sum(1 / (X_work[col] + epsilon) for col in available_amenities)\n",
    "        \n",
    "        return X_work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f8df7",
   "metadata": {},
   "source": [
    "The SmartCategoricalEncoder automatically chooses the best encoding strategy based on each categorical variable's characteristics (cardnality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "336528c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartCategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Effective categorical encoding without overcomplication\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.encodings = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        cat_cols = X.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in cat_cols:\n",
    "            unique_vals = X[col].unique()\n",
    "            \n",
    "            if len(unique_vals) > 10:\n",
    "                # Frequency encode sub_area (market activity indicator)\n",
    "                freq_map = X[col].value_counts().to_dict()\n",
    "                self.encodings[col] = ('frequency', freq_map)\n",
    "            elif len(unique_vals) <= 5:\n",
    "                # One-hot for low cardinality\n",
    "                self.encodings[col] = ('onehot', unique_vals)\n",
    "            else:\n",
    "                # Ordinal for medium cardinality\n",
    "                encoding_map = {val: i for i, val in enumerate(unique_vals)}\n",
    "                self.encodings[col] = ('ordinal', encoding_map)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_work = X.copy()\n",
    "        \n",
    "        for col, (method, mapping) in self.encodings.items():\n",
    "            if col not in X_work.columns:\n",
    "                continue\n",
    "                \n",
    "            if method == 'frequency':\n",
    "                X_work[col] = X_work[col].map(mapping).fillna(1)\n",
    "            \n",
    "            elif method == 'ordinal':\n",
    "                X_work[col] = X_work[col].map(mapping).fillna(-1)\n",
    "            \n",
    "            elif method == 'onehot':\n",
    "                # Simple one-hot encoding\n",
    "                for val in mapping:\n",
    "                    X_work[f\"{col}_{val}\"] = (X_work[col] == val).astype(int)\n",
    "                X_work = X_work.drop(columns=[col])\n",
    "        \n",
    "        return X_work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e177bf0",
   "metadata": {},
   "source": [
    "FOCUSED PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "750039e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "focused_preprocessor = Pipeline([\n",
    "    ('imputer', HybridImputer()),\n",
    "    ('feature_engineer', SelectiveFeatureEngineer()), \n",
    "    ('encoder', SmartCategoricalEncoder()),\n",
    "    ('scaler', RobustScaler()) # Standardize all features after encoding\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8b9f2",
   "metadata": {},
   "source": [
    "MODEL PIPELINE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eaaf6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('preprocessor', focused_preprocessor),\n",
    "    ('regressor', Ridge(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbc4602",
   "metadata": {},
   "source": [
    "FIT THE PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fe485ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting advanced preprocessing pipeline...\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting advanced preprocessing pipeline...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "test_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d54114cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types for reporting\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c00c925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Test RMSE: 0.502 (log scale)\n",
      "\n",
      "============================================================\n",
      "ADVANCED PREPROCESSING BENEFITS:\n",
      "============================================================\n",
      "Handles missing values (median/mode imputation)\n",
      "Converts categorical data to numeric (smart encoding)\n",
      "Creates new predictive features (feature engineering)\n",
      "Standardizes feature scales\n",
      "Uses ALL available features (not just complete numeric)\n",
      "Prevents data leakage (fit only on training data)\n",
      "Robust to new data (handles unseen categories)\n",
      "Focused feature engineering (avoids overfitting)\n"
     ]
    }
   ],
   "source": [
    "print(f\"   Test RMSE: {test_rmse:.3f} (log scale)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADVANCED PREPROCESSING BENEFITS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Handles missing values (median/mode imputation)\")\n",
    "print(\"Converts categorical data to numeric (smart encoding)\")\n",
    "print(\"Creates new predictive features (feature engineering)\")\n",
    "print(\"Standardizes feature scales\") \n",
    "print(\"Uses ALL available features (not just complete numeric)\")\n",
    "print(\"Prevents data leakage (fit only on training data)\")\n",
    "print(\"Robust to new data (handles unseen categories)\")\n",
    "print(\"Focused feature engineering (avoids overfitting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "053f4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate residuals\n",
    "# residuals = y_test - test_pred\n",
    "\n",
    "# # 1. Check homoscedasticity\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(test_pred, residuals, alpha=0.6)\n",
    "# plt.xlabel(\"Fitted Values (Predictions)\")\n",
    "# plt.ylabel(\"Residuals\")\n",
    "# plt.title(\"Residuals vs Fitted\")\n",
    "# plt.axhline(y=0, color='red', linestyle='--')\n",
    "\n",
    "# # 2. Check normality of residuals\n",
    "# plt.subplot(1, 2, 2)\n",
    "# from scipy import stats\n",
    "# stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "# plt.title(\"Q-Q Plot - Residual Normality\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d22584",
   "metadata": {},
   "source": [
    "Let's compare the results (by converting log scale to $ scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "791c4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"baseline_rmse.txt\", \"r\") as f:\n",
    "#     baseline_rmse_dollars = float(f.read().strip())\n",
    "\n",
    "with open(\"baseline_rmse_fine_tuned.txt\", \"r\") as f:\n",
    "    baseline_rmse_fine_tuned_dollars = float(f.read().strip())\n",
    "    \n",
    "# Convert back to dollar scale for interpretation\n",
    "test_pred_dollars = np.expm1(test_pred)\n",
    "y_test_dollars = np.expm1(y_test)\n",
    "\n",
    "test_rmse_dollars = np.sqrt(mean_squared_error(y_test_dollars, test_pred_dollars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5351594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Percentage improvement over baseline with fine-tuned model: 24.10%\n",
      "\n",
      "Dollar amount improvement over baseline with fine-tuned model: $1,060,604.07\n"
     ]
    }
   ],
   "source": [
    "# percent_improvement = (baseline_rmse_dollars - test_rmse_dollars) / baseline_rmse_dollars * 100\n",
    "# dollars_improvement = baseline_rmse_dollars - test_rmse_dollars\n",
    "# print(f\"\\nPercentage improvement over baseline: {percent_improvement:.2f}%\")\n",
    "# print(f\"\\nDollar amount improvement over baseline: ${dollars_improvement:.2f}\")\n",
    "\n",
    "percent_improvement = (baseline_rmse_fine_tuned_dollars - test_rmse_dollars) / baseline_rmse_fine_tuned_dollars * 100\n",
    "dollars_improvement = baseline_rmse_fine_tuned_dollars - test_rmse_dollars\n",
    "print(f\"\\nPercentage improvement over baseline with fine-tuned model: {percent_improvement:.2f}%\")\n",
    "print(f\"\\nDollar amount improvement over baseline with fine-tuned model: ${dollars_improvement:,.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
